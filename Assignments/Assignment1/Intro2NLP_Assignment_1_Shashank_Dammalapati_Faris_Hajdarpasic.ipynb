{"cells":[{"cell_type":"markdown","id":"5a325897","metadata":{"id":"5a325897"},"source":["# Introduction to Natural Language Processing: Assignment 1\n","\n","In this assignment we'll practice tokenization, lemmatization and stemming\n","\n","- Please comment your code\n","- Submissions are due Thursday at 23:59 and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 14.11.2023, at 23:59)**\n","- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n","- Please submit **ONLY** the Jupyter Notebook file.\n","- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n","\n","Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"]},{"cell_type":"markdown","id":"0cd8bf33","metadata":{"id":"0cd8bf33"},"source":["### Task 1.1 (3 points)\n","\n","Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n","1. num_words: The number of words in string\n","2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n","\n","**Hint:** The string can be a single word or a sentence and\n"," can contain some special charecters, such as: \"!\", \",\", \":\""]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" **Hint:** The string can be a single word i.e. 1 word, or a sentence and can contain some special charecters, such as: !,,, : \n","['', '**Hint:**', 'The', 'string', 'can', 'be', 'a', 'single', 'word', 'i.e.', '1', 'word,', 'or', 'a', 'sentence', 'and', 'can', 'contain', 'some', 'special', 'charecters,', 'such', 'as:', '!,,,', ':', '']\n"]}],"source":["test_string = \" **Hint:** The string can be a single word i.e. 1 word, or a sentence and can contain some special charecters, such as: !,,, : \"\n","print(test_string)\n","print(test_string.split(\" \"))"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["127\n"]}],"source":["def character_tokenize(string):\n","    # Considering space as a token as well\n","    return list(string)\n","\n","print(len(character_tokenize(test_string)))"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}],"source":["def word_vocab():\n","    # This contains the subset of utf-8 which i consider as a word vocabulary for this assignment\n","    numbers_range = range(ord('0'), ord('9') + 1)\n","    upper_case_range = range(ord('A'), ord('Z') + 1)\n","    lower_case_range = range(ord('a'), ord('z') + 1)\n","    word_vocab_ids = list(numbers_range) + list(upper_case_range) + list(lower_case_range)\n","    word_vocab = [chr(i) for i in word_vocab_ids]\n","    return word_vocab\n","\n","print(word_vocab())\n","\n","word_vocab_ = word_vocab()"]},{"cell_type":"code","execution_count":84,"id":"f14f3124","metadata":{"id":"f14f3124"},"outputs":[{"name":"stdout","output_type":"stream","text":[" **Hint:** The string can be a single word i.e. 1 word, or a sentence and can contain some special charecters, such as: !,,, :  \n"," ['Hint', 'The', 'string', 'can', 'be', 'a', 'single', 'word', 'i', 'e', '1', 'word', 'or', 'a', 'sentence', 'and', 'can', 'contain', 'some', 'special', 'charecters', 'such', 'as'] \n"," Hint The string can be a single word i e 1 word or a sentence and can contain some special charecters such as \n"," num_words: 23 and num_tokens: 127 respectively\n"]}],"source":["def extract_words_tokens(any_string):\n","    # Returns the number of words and tokens in a string\n","    # Tokenization based on characters\n","\n","    num_tokens = len(character_tokenize(any_string))\n","\n","    # Only retaining word_vocab characters for word counting\n","    word_vocab_ = word_vocab()\n","    new_string = \"\"\n","    for char in any_string:\n","        if char in word_vocab_:\n","            new_string += char\n","        else:\n","            new_string += \" \"\n","\n","    # Removing white spaces at the beginning and end of the string and also removing multiple white spaces\n","    # new_string = new_string.strip() # Removes white spaces at the beginning and end of the string\n","    new_string = \" \".join(new_string.split()) # Split removes trailing and leading and extra white spaces, join adds a single white space between words\n","    each_words = new_string.split(\" \")\n","    num_words = len(each_words)\n","\n","\n","    return(print(any_string, \"\\n\",each_words, \"\\n\",new_string, \"\\n\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))\n","\n","extract_words_tokens(test_string)"]},{"cell_type":"markdown","metadata":{},"source":["This method can't solve for example \"i.e.\" - it broke that down into 2 words. I didnt want to add a special case for this. I have to read and figure out hwo to deal with such special cases."]},{"cell_type":"markdown","id":"a4b05add","metadata":{"id":"a4b05add"},"source":["### Task 1.2 (4 points)\n","\n","Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n","\n","**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('first', '1'), ('tenth', '10'), ('hundredth', '100'), ('thousandth', '1000'), ('millionth', '1000000'), ('billionth', '1000000000'), ('eleventh', '11'), ('twelfth', '12'), ('thirteenth', '13'), ('fourteenth', '14')]\n"]}],"source":["def lemmatization_dictionary(file):\n","    '''\n","    Input: file name of the text with lemma and word pairs\n","    FORMAT:\n","    accede\tacceded\n","    accede\taccedes\n","    accede\tacceding\n","    LEFT: lemma\n","    RIGHT: word\n","    '''\n","    dict = {}\n","    with open(file) as f:\n","        for line in f:\n","            line = \" \".join(line.split())\n","            val, key = line.split()\n","            dict[key] = val\n","        dict['first'] = \"1\"\n","    return dict\n","\n","print(list(lemmatization_dictionary(\"lemmatization-en.txt\").items())[:10])\n"]},{"cell_type":"code","execution_count":86,"id":"a12f48ff","metadata":{"id":"a12f48ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["I be go to the market \n"]}],"source":["def lemmatize(any_string, file_name):\n","    '''\n","    Input: any string and the file name of the text with lemma and word pairs\n","    Output: lemmatized - any string\n","    '''\n","    dict = lemmatization_dictionary(file_name)\n","    new_string = \"\"\n","    for word in any_string.split(\" \"):\n","        if word in dict:\n","            new_string += dict[word] + \" \"\n","        else:\n","            new_string += word + \" \"\n","    return new_string\n","\n","print(lemmatize(\"I am going to the market\", \"lemmatization-en.txt\"))\n"]},{"cell_type":"markdown","id":"f266bdc4","metadata":{"id":"f266bdc4"},"source":["### Task 1.3 (3 points)\n","\n","Write a function `stemmer(string)` that takes a string as input and returns a string conaining only its stem.\n","\n","Create rules for the following forms of the verbs, Here is one example:\n","\n","- (Infinitive form) >> study - studi\n","- (Present simple tense: Third person) >> studies - studi\n","- (Continuous tense) >> studying - studi\n","- (Past simple tense) >> studied - studi\n","\n","**Hint:** The string can be a single word or a sentence and\n"," can contain some special charecters, such as: \"!\", \",\", \":\""]},{"cell_type":"code","execution_count":90,"id":"0b5c587b","metadata":{"id":"0b5c587b"},"outputs":[{"name":"stdout","output_type":"stream","text":["stemmed string of : I am going to the car shows that are happening in the cities of Mars colonized by Elon Musk is \n"," I am go to the car show that are happen in the citi of Mar coloniz by Elon Musk is \n"]}],"source":["def stemmer(word):\n","    ''' \n","    Input: any word\n","    Output: stemmed word\n","    RULES: End of word is y, ing, ed, es, s, ies, ied then remove it or replace it with i if preceded by a consonant\n","    '''\n","    # Stemming rules\n","    if len(word) <= 2:\n","        return word\n","\n","    elif word[-1] == 'y':\n","        return word[:-1] + 'i'\n","    elif word[-3:] == 'ing':\n","        return word[:-3]\n","    elif word[-2:] == 'ed':\n","        return word[:-2]\n","    elif word[-2:] == 'es':\n","        return word[:-2]\n","    elif word[-1] == 's':\n","        return word[:-1]\n","    elif word[-3:] == 'ies':\n","        return word[:-3] + 'i'\n","    elif word[-3:] == 'ied':\n","        return word[:-3] + 'i'\n","    else:\n","        return word\n","\n","def words_tokens(any_string):\n","    # Returns the number of words and tokens in a string\n","    # Tokenization based on characters\n","\n","    num_tokens = len(character_tokenize(any_string))\n","\n","    # Only retaining word_vocab characters for word counting\n","    # word_vocab_ = word_vocab()\n","    new_string = \"\"\n","    for char in any_string:\n","        if char in word_vocab_:\n","            new_string += char\n","        else:\n","            new_string += \" \"\n","\n","    # Removing white spaces at the beginning and end of the string and also removing multiple white spaces\n","    new_string = \" \".join(new_string.split())  # Split removes trailing and leading and extra white spaces, join adds a single white space between words\n","    each_words = new_string.split(\" \")\n","    num_words = len(each_words)\n","\n","    return new_string\n","\n","def stemmed_string(any_string):\n","    ''' \n","    Input: any string\n","    Output: stemmed string\n","    RULES: End of word is y, ing, ed, es, s, ies, ied then remove it or replace it with i if preceded by a consonant\n","    '''\n","    # Note - remove all special characters \n","    new_string = words_tokens(any_string)\n","    stemmed_string = \"\"\n","    for word in new_string.split(\" \"):\n","        stemmed_string += stemmer(word) + \" \"\n","    return stemmed_string\n","\n","print(\"stemmed string of : I am going to the car shows that are happening in the cities of Mars colonized by Elon Musk is\", \"\\n\",stemmed_string(\"I am going to the car shows that are happening in the cities of Mars colonized by Elon Musk is\"))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
